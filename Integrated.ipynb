{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWrqXbbTx_rU",
    "outputId": "749d923e-9553-46c3-a0db-1a9456cb10d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /home/mridul/anaconda3/lib/python3.8/site-packages (0.25.1)\n",
      "Requirement already satisfied: SpeechRecognition in /home/mridul/anaconda3/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: gradio in /home/mridul/anaconda3/lib/python3.8/site-packages (1.7.7)\n",
      "Requirement already satisfied: scipy in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (1.5.2)\n",
      "Requirement already satisfied: requests in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (2.24.0)\n",
      "Requirement already satisfied: pycryptodome in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (3.10.1)\n",
      "Requirement already satisfied: flask-cachebuster in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: analytics-python in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (1.2.9)\n",
      "Requirement already satisfied: Flask-Login in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: markdown2 in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (2.4.0)\n",
      "Requirement already satisfied: ffmpy in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: pandas in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (1.1.3)\n",
      "Requirement already satisfied: Flask>=1.1.1 in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (1.1.2)\n",
      "Requirement already satisfied: matplotlib in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (3.3.2)\n",
      "Requirement already satisfied: numpy in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (1.19.2)\n",
      "Requirement already satisfied: paramiko in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (2.7.2)\n",
      "Requirement already satisfied: pillow in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (8.0.1)\n",
      "Requirement already satisfied: Flask-Cors>=3.0.8 in /home/mridul/anaconda3/lib/python3.8/site-packages (from gradio) (3.0.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/mridul/anaconda3/lib/python3.8/site-packages (from requests->gradio) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mridul/anaconda3/lib/python3.8/site-packages (from requests->gradio) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mridul/anaconda3/lib/python3.8/site-packages (from requests->gradio) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/mridul/anaconda3/lib/python3.8/site-packages (from requests->gradio) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /home/mridul/anaconda3/lib/python3.8/site-packages (from analytics-python->gradio) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/mridul/anaconda3/lib/python3.8/site-packages (from analytics-python->gradio) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/mridul/anaconda3/lib/python3.8/site-packages (from pandas->gradio) (2020.1)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /home/mridul/anaconda3/lib/python3.8/site-packages (from Flask>=1.1.1->gradio) (1.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /home/mridul/anaconda3/lib/python3.8/site-packages (from Flask>=1.1.1->gradio) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /home/mridul/anaconda3/lib/python3.8/site-packages (from Flask>=1.1.1->gradio) (2.11.2)\n",
      "Requirement already satisfied: click>=5.1 in /home/mridul/anaconda3/lib/python3.8/site-packages (from Flask>=1.1.1->gradio) (7.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mridul/anaconda3/lib/python3.8/site-packages (from matplotlib->gradio) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mridul/anaconda3/lib/python3.8/site-packages (from matplotlib->gradio) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/mridul/anaconda3/lib/python3.8/site-packages (from matplotlib->gradio) (2.4.7)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /home/mridul/anaconda3/lib/python3.8/site-packages (from paramiko->gradio) (3.2.0)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /home/mridul/anaconda3/lib/python3.8/site-packages (from paramiko->gradio) (1.4.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/mridul/anaconda3/lib/python3.8/site-packages (from paramiko->gradio) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/mridul/anaconda3/lib/python3.8/site-packages (from Jinja2>=2.10.1->Flask>=1.1.1->gradio) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.1 in /home/mridul/anaconda3/lib/python3.8/site-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.14.3)\n",
      "Requirement already satisfied: pycparser in /home/mridul/anaconda3/lib/python3.8/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.20)\n",
      "Requirement already satisfied: pickle5 in /home/mridul/anaconda3/lib/python3.8/site-packages (0.0.11)\n",
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub\n",
    "!pip install SpeechRecognition\n",
    "!pip install gradio\n",
    "!pip install pickle5\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import sys\n",
    "sys.path.append('./utils/')\n",
    "from prepare_data import *\n",
    "from model_helper import *\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "import time\n",
    "from numpy import load\n",
    "from keras.models import model_from_json\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "import moviepy.editor\n",
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import pickle5 as pickle\n",
    "import re\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gradio as gr\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w_eX-F5bJPfr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import sklearn\n",
    "import pandas\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "from keras import layers\n",
    "from keras import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import *\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.applications import *\n",
    "from keras import regularizers\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m7-dyChty7cl"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AOi68x4syS9v"
   },
   "outputs": [],
   "source": [
    "class ABLSTM(object):\n",
    "    def __init__(self, config):\n",
    "        self.max_len = config[\"max_len\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"]\n",
    "        self.embedding_size = config[\"embedding_size\"]\n",
    "        self.n_class = config[\"n_class\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "\n",
    "        # placeholder\n",
    "        self.x = tf.compat.v1.placeholder(tf.int32, [None, self.max_len])\n",
    "        self.label = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "        self.keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "\n",
    "    def build_graph(self):\n",
    "        print(\"building graph\")\n",
    "        # Word embedding\n",
    "        embeddings_var = tf.Variable(tf.random.uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n",
    "                                     trainable=True)\n",
    "        batch_embedded = tf.nn.embedding_lookup(params=embeddings_var, ids=self.x)\n",
    "\n",
    "        rnn_outputs, _ = bi_rnn(tf.compat.v1.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
    "                                tf.compat.v1.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
    "                                inputs=batch_embedded, dtype=tf.float32)\n",
    "\n",
    "        fw_outputs, bw_outputs = rnn_outputs\n",
    "\n",
    "        W = tf.Variable(tf.random.normal([self.hidden_size], stddev=0.1))\n",
    "        H = fw_outputs + bw_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
    "        M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
    "\n",
    "        self.alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, self.hidden_size]),\n",
    "                                                        tf.reshape(W, [-1, 1])),\n",
    "                                              (-1, self.max_len)))  # batch_size x seq_len\n",
    "        r = tf.matmul(tf.transpose(a=H, perm=[0, 2, 1]),\n",
    "                      tf.reshape(self.alpha, [-1, self.max_len, 1]))\n",
    "        r = tf.squeeze(r)\n",
    "        h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
    "\n",
    "        h_drop = tf.nn.dropout(h_star, rate=1 - (self.keep_prob))\n",
    "\n",
    "        # Fully connected layerï¼ˆdense layer)\n",
    "        FC_W = tf.Variable(tf.random.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n",
    "        FC_b = tf.Variable(tf.constant(0., shape=[self.n_class]))\n",
    "        y_hat = tf.compat.v1.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
    "\n",
    "        self.loss = tf.reduce_mean(\n",
    "            input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=self.label))\n",
    "\n",
    "        # prediction\n",
    "        self.prediction = tf.argmax(input=tf.nn.softmax(y_hat), axis=1)\n",
    "\n",
    "        # optimization\n",
    "        loss_to_minimize = self.loss\n",
    "        tvars = tf.compat.v1.trainable_variables()\n",
    "        gradients = tf.gradients(ys=loss_to_minimize, xs=tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n",
    "                                                       name='train_step')\n",
    "        print(\"graph built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-0g5doA8yhQZ"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"max_len\": 100,\n",
    "        \"hidden_size\": 64,\n",
    "        \"vocab_size\": 161177,\n",
    "        \"embedding_size\": 128,\n",
    "        \"n_class\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 4,\n",
    "        \"train_epoch\": 5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrQNksEoy2RP",
    "outputId": "04df70a9-df91-481b-d4c2-fe37d20ba742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building graph\n",
      "graph built successfully!\n"
     ]
    }
   ],
   "source": [
    "classifier = ABLSTM(config)\n",
    "classifier.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYLFFy1BPu6k"
   },
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XVIl8SOQxu3T"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.train.Saver().restore(sess, tf.compat.v1.train.latest_checkpoint('./attn_bilstm/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V5M2vTsVhl5T"
   },
   "outputs": [],
   "source": [
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    aud1 = []\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "                #aud1.append(whole_text)\n",
    "    aud1.append(whole_text)           \n",
    "    # return the text for all chunks detected\n",
    "    return aud1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhX-00mehTtG",
    "outputId": "a817c6af-f389-434e-a2bc-9baeb0516411"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mridul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mridul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mridul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "r = sr.Recognizer()\n",
    "lem = WordNetLemmatizer()\n",
    "with open('./data/tokenizer.pickle', 'rb') as handle:\n",
    "  tokenizer = pickle.load(handle)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RFmMFgGU7fng"
   },
   "outputs": [],
   "source": [
    "def audio_prediction(video):\n",
    "  video=moviepy.editor.VideoFileClip(video)\n",
    "  audio=video.audio\n",
    "  audio.write_audiofile('./sample_video/audio/audio.wav')\n",
    "\n",
    "  audio_path = './sample_video/audio/audio.wav'\n",
    "\n",
    "\n",
    "  aud1 = get_large_audio_transcription(audio_path)\n",
    "\n",
    "  aud1_str = aud1[0]\n",
    "  sentences = aud1_str.split('.')\n",
    "  all_sentences = []\n",
    "  for sentence in sentences:\n",
    "    sentence_new = sentence.lower()\n",
    "    sentence_new = sentence_new.strip()\n",
    "    all_sentences.append(sentence_new)\n",
    "\n",
    "  all_sentences1 = []\n",
    "  for sentence in all_sentences:\n",
    "    if len(sentence) > 1:\n",
    "      all_sentences1.append(sentence)\n",
    "\n",
    "  length = []\n",
    "  total_words = 0\n",
    "  for tweet in all_sentences1:\n",
    "    len_tweet = len(tweet.split(' '))\n",
    "    total_words += len_tweet\n",
    "    length.append(len_tweet)\n",
    "\n",
    "  video_duration = int(video.duration)\n",
    "  factor = 0\n",
    "  if total_words > 0:\n",
    "    factor = video_duration/total_words\n",
    "\n",
    "  starting_time = []\n",
    "  ending_time = []\n",
    "  ending_val = 0\n",
    "  starting_time.append(0)\n",
    "  for l in length:\n",
    "    ending_val = ending_val + l*factor\n",
    "    starting_time.append(ending_val)\n",
    "    ending_time.append(ending_val)\n",
    "  starting_time = starting_time[0:-1]\n",
    "\n",
    "  time_stamps = []\n",
    "  for i in range(len(starting_time)):\n",
    "    start = starting_time[i]\n",
    "    end = ending_time[i]\n",
    "    h1 = int(start // 3600)\n",
    "    m1 = int(start // 60)\n",
    "    s1 = int(start - h1*3600 - m1 * 60)\n",
    "    h2 = int(end // 3600)\n",
    "    m2 = int(end // 60)\n",
    "    s2 = int(end - h2*3600 - m2 * 60)\n",
    "    generated_time_stamp = str(h1)+':'+str(m1)+':'+str(s1)+'-'+str(h2)+':'+str(m2)+':'+str(s2)\n",
    "    time_stamps.append(generated_time_stamp)\n",
    "\n",
    "  all_sentences_final = []\n",
    "  for sentence in all_sentences1:\n",
    "    new_sentence = \"\"\n",
    "    all_words = word_tokenize(sentence)\n",
    "    for word in all_words:\n",
    "      if word not in stop_words:\n",
    "        new_word = lem.lemmatize(word)\n",
    "        new_sentence = new_sentence + ' ' + word\n",
    "    new_sentence = new_sentence[1:]\n",
    "    all_sentences_final.append(new_sentence)\n",
    "\n",
    "  map = dict(zip(all_sentences_final, range(len(all_sentences_final))))\n",
    "\n",
    "  preprocessed_sentences = []\n",
    "  for sentence in all_sentences_final:\n",
    "    if len(sentence) > 1:\n",
    "      preprocessed_sentences.append(sentence)\n",
    "\n",
    "  df = DataFrame(preprocessed_sentences, columns=['tweet'])\n",
    "  df.dropna(axis=0, inplace=True)\n",
    "  x_tf=df['tweet']\n",
    "\n",
    "  x_tf.dropna(axis=0,inplace=True)\n",
    "  prediction_data = pad_sequences(tokenizer.texts_to_sequences(x_tf), maxlen=100)\n",
    "  prediction_output = np.zeros(len(prediction_data))\n",
    "  feed_dict = make_test_feed_dict(classifier, (prediction_data, prediction_output))\n",
    "  prediction = sess.run(classifier.prediction, feed_dict)\n",
    "  percentage_hate = np.count_nonzero(prediction == 1)\n",
    "\n",
    "  result = 'The hate sentences are as follows:\\n'\n",
    "  for i in range(len(prediction)):\n",
    "    if prediction[i] == 1:\n",
    "      index = map[preprocessed_sentences[i]]\n",
    "      result = result + all_sentences1[index] +' (' + time_stamps[index] + ')\\n'\n",
    "\n",
    "  if percentage_hate > 0.04:\n",
    "    return 'Violent', result\n",
    "  else:\n",
    "    return 'Non-Violent', 'No time stamps as video is non violent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iik-itbM0dut"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PEhfgWWpWBpE"
   },
   "outputs": [],
   "source": [
    "def video_to_frames(input_loc, output_loc,video_name):\n",
    "\n",
    "    count = 0\n",
    "    cap = cv2.VideoCapture(input_loc)   # capturing the video from the given path\n",
    "    frameRate = cap.get(5) #frame rate\n",
    "    print (frameRate)\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(10) #current frame number\n",
    "        # print(frameId)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if (ret != True):\n",
    "            break\n",
    "        if (frameId % math.floor(frameRate) == 0):\n",
    "            frame = cv2.resize(frame, (224,224))\n",
    "            filename = video_name + \"frame%d.jpg\" % count;count+=1\n",
    "            cv2.imwrite(output_loc + filename, frame)\n",
    "            \n",
    "    cap.release()\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yhJiDkIXWJLg"
   },
   "outputs": [],
   "source": [
    "def create_seq_single_video(frames):\n",
    "    \n",
    "    print('+++ Creating Sequence... +++')\n",
    "    vid = []\n",
    "#     frames = np.array(frames)\n",
    "    i = 0\n",
    "    while i < len(frames):\n",
    "        vid.append(frames[i:i+30])\n",
    "        i = i+30\n",
    "    vid = np.asarray(vid)\n",
    "    \n",
    "    return vid\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "89lBGlRcWN1a"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_lstm(features, frames):\n",
    "    frames = np.asarray(frames)\n",
    "    v_features = features[0:30*math.floor(frames.shape[0]/30)]\n",
    "    \n",
    "    print(\"Video Features Shape: \", v_features.shape)\n",
    "    \n",
    "    vid = create_seq_single_video(v_features)\n",
    "    \n",
    "    test_x = vid\n",
    "    print(\"Total data: \", test_x.shape)\n",
    "    test_x = np.reshape(test_x, (test_x.shape[0],test_x.shape[1],np.prod(test_x.shape[2:])))\n",
    "    print(\"(LSTM) After Rehshape: \", test_x.shape)\n",
    "    \n",
    "    return test_x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Zci5SnDzWN4H"
   },
   "outputs": [],
   "source": [
    "def feature_extract(frames, model):\n",
    "    \n",
    "    print(\"+++ Extracting feature... +++\")\n",
    "    \n",
    "    frames = np.asarray(frames)\n",
    "    \n",
    "    print(\"Before Feaure extraction: \")\n",
    "    print(frames.shape)\n",
    "    \n",
    "    all_data = frames\n",
    "    print(\"Adding all data: \", all_data.shape)\n",
    "    \n",
    "    #creates feature descriptors\n",
    "    all_data = all_data.astype('float64')\n",
    "    desc = preprocess_input(all_data)\n",
    "    if(model == 'resnet50'):\n",
    "        loaded_model = resnet50.ResNet50(input_shape=(224,224,3), include_top=False)\n",
    "    elif(model == 'vgg19'):\n",
    "        loaded_model = VGG19(input_shape=(224,224,3), include_top=False)\n",
    "    elif(model == 'vgg16'):\n",
    "        loaded_model = VGG16(input_shape=(224,224,3), include_top=False)\n",
    "    else:\n",
    "        print(\"Please give model name - 'resnet50', 'vgg19', 'vgg16'\")\n",
    "        \n",
    "    loaded_model = Model(loaded_model.input,loaded_model.output)\n",
    "    features = loaded_model.predict(desc,batch_size=10,verbose=1)\n",
    "\n",
    "    print (\"After Feature extraction: \", features.shape)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xO_ulxfPXBhm"
   },
   "outputs": [],
   "source": [
    "frames_output_path = './sample_video/frames_output/'\n",
    "v_path = r\"./sample_video/frames_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sJWf3Nv6Idbq"
   },
   "outputs": [],
   "source": [
    "def video_prediction(video):\n",
    "  video_file = moviepy.editor.VideoFileClip(video)\n",
    "  video_duration = int(video_file.duration)\n",
    "  print(\"+++ Removing all the frames from the source folder... +++\")\n",
    "  files = glob.glob(r\"./sample_video/frames_output/*\")\n",
    "  for f in files:\n",
    "      os.remove(f)\n",
    "\n",
    "  video_to_frames(video,frames_output_path,'video')\n",
    "  out_frames = []\n",
    "\n",
    "  for frame in os.listdir(v_path):\n",
    "      frame = cv2.imread(os.path.join(v_path,frame))\n",
    "      out_frames.append(frame)\n",
    "\n",
    "\n",
    "  print(\"++++++++++ Resnet50 Features Extraction ++++++++\")\n",
    "  features = feature_extract(out_frames, 'resnet50')\n",
    "\n",
    "  print(\"++++++++++ Preprocesing LSTM +++++++++++++++++++\")\n",
    "  test_x = preprocess_lstm(features, out_frames)\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(50, input_shape=(test_x.shape[1],test_x.shape[2]), return_sequences=False, kernel_regularizer=regularizers.l2(0.01)))\n",
    "  model.add(Dense(1,activation='sigmoid'))\n",
    "  #model.load_weights('resnet_LSTM_v1.h5')\n",
    "  model.load_weights('./videoModel/resnet_LSTM_wwe.h5')\n",
    "  model.summary()\n",
    "  optimizer = optimizers.Adam(lr=0.001,decay=0.004)\n",
    "  model.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "\n",
    "  print(\"+++ Generating result... +++\")\n",
    "      \n",
    "  pred = model.predict(test_x)\n",
    "\n",
    "  final_out = [1 if x > 0.5 else 0 for x in pred ]\n",
    "\n",
    "\n",
    "  factor = video_duration/len(final_out)\n",
    "\n",
    "  time_stamps = []\n",
    "  start = end = 0\n",
    "  flag = False\n",
    "  for i in range(len(final_out)):\n",
    "    if final_out[i] == 1 and flag == False:\n",
    "      start = i*factor\n",
    "      flag = True\n",
    "    elif final_out[i] == 1 and flag == True:\n",
    "      continue\n",
    "    elif final_out[i] == 0 and flag == True:\n",
    "      end = i*factor\n",
    "      flag = False\n",
    "      time_stamps.append((start, end))\n",
    "    else:\n",
    "      continue\n",
    "  if final_out[-1] == 1 and flag == True:\n",
    "    time_stamps.append((start, video_duration))\n",
    "\n",
    "  result = 'The hate time stamps are as follows:\\n'\n",
    "\n",
    "  for (start, end) in time_stamps:\n",
    "    h1 = int(start // 3600)\n",
    "    m1 = int(start // 60)\n",
    "    s1 = int(start - h1*3600 - m1 * 60)\n",
    "    h2 = int(end // 3600)\n",
    "    m2 = int(end // 60)\n",
    "    s2 = int(end - h2*3600 - m2 * 60)\n",
    "    time_stamp = str(h1)+':'+str(m1)+':'+str(s1)+'-'+str(h2)+':'+str(m2)+':'+str(s2)+'\\n'\n",
    "    result = result + time_stamp\n",
    "\n",
    "  print(\"+++ Final Output... +++\")\n",
    "  print(pd.DataFrame(final_out)[0].value_counts())\n",
    "\n",
    "\n",
    "  df = pd.DataFrame(final_out)\n",
    "  video_result = ''\n",
    "  try:\n",
    "    string1 = \"Percentage of non-violent sequence of frames : {0}\".format(list(df.value_counts(normalize=True)[1].values*100)[0])\n",
    "    string2 = \"Percentage of violent sequence of frames : {0}\".format(list(df.value_counts(normalize=True)[0].values*100)[0])\n",
    "    print(string1)\n",
    "    print(string2)\n",
    "    video_result = string1 + '\\n' + string2\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    if list(df.value_counts(normalize=True)[1].values*100)[0] > 80:\n",
    "      return \"NON-VIOLENT\", result\n",
    "    else:\n",
    "      return \"VIOLENT\", result\n",
    "  except:\n",
    "    return \"VIOLENT\", 'No time stamps as video is not violent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "S-9Op1J4Fc0q"
   },
   "outputs": [],
   "source": [
    "iface = gr.Interface([audio_prediction, video_prediction], inputs=\"video\", outputs=['text', 'text'], title=\"Violence Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759
    },
    "id": "oyUnLKtPFg5o",
    "outputId": "f1b78c62-20fc-45ef-cc4d-722e9d08c486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7860/\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7860/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1584390370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 0/1242 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./sample_video/audio/audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "+++ Removing all the frames from the source folder... +++\n",
      "25.0\n",
      "Done!\n",
      "++++++++++ Resnet50 Features Extraction ++++++++\n",
      "+++ Extracting feature... +++\n",
      "Before Feaure extraction: \n",
      "(1405, 224, 224, 3)\n",
      "Adding all data:  (1405, 224, 224, 3)\n",
      "After Feature extraction:  (1405, 7, 7, 2048)\n",
      "++++++++++ Preprocesing LSTM +++++++++++++++++++\n",
      "Video Features Shape:  (1380, 7, 7, 2048)\n",
      "+++ Creating Sequence... +++\n",
      "Total data:  (46, 30, 7, 7, 2048)\n",
      "(LSTM) After Rehshape:  (46, 30, 100352)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                20080600  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 20,080,651\n",
      "Trainable params: 20,080,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "+++ Generating result... +++\n",
      "+++ Final Output... +++\n",
      "0    46\n",
      "Name: 0, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 0/1165 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./sample_video/audio/audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-chunks/chunk1.wav : Cheating case hamen sirf got the west. \n",
      "audio-chunks/chunk2.wav : I think is what i am. \n",
      "audio-chunks/chunk3.wav : Haters. \n",
      "audio-chunks/chunk4.wav : Something. \n",
      "audio-chunks/chunk5.wav : Something where that did the tremendous hated there was a tremendous that we have to get you the. \n",
      "audio-chunks/chunk6.wav : There is a b for ball. \n",
      "audio-chunks/chunk7.wav : Hatred of us. \n",
      "audio-chunks/chunk8.wav : Vishwamitra. \n",
      "audio-chunks/chunk9.wav : Traffic dada. \n",
      "audio-chunks/chunk10.wav : Ok you get another for letter write. \n",
      "audio-chunks/chunk11.wav : The figure that out there is a tremendous. \n",
      "audio-chunks/chunk12.wav : Hatred and we have to be very vigilant. \n",
      "audio-chunks/chunk13.wav : Best be very careful. \n",
      "audio-chunks/chunk14.wav : We can allow people coming into this country who have the sacred. \n",
      "audio-chunks/chunk15.wav : Abhi an arrest people. \n",
      "audio-chunks/chunk16.wav : That is there war between the west and radical islam was the question is why it is very hard to find it very hard to to separate because you don't know who's who. \n",
      "+++ Removing all the frames from the source folder... +++\n",
      "30.0\n",
      "Done!\n",
      "++++++++++ Resnet50 Features Extraction ++++++++\n",
      "+++ Extracting feature... +++\n",
      "Before Feaure extraction: \n",
      "(1583, 224, 224, 3)\n",
      "Adding all data:  (1583, 224, 224, 3)\n",
      "After Feature extraction:  (1583, 7, 7, 2048)\n",
      "++++++++++ Preprocesing LSTM +++++++++++++++++++\n",
      "Video Features Shape:  (1560, 7, 7, 2048)\n",
      "+++ Creating Sequence... +++\n",
      "Total data:  (52, 30, 7, 7, 2048)\n",
      "(LSTM) After Rehshape:  (52, 30, 100352)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                20080600  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 20,080,651\n",
      "Trainable params: 20,080,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "+++ Generating result... +++\n",
      "+++ Final Output... +++\n",
      "1    52\n",
      "Name: 0, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 0/326 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./sample_video/audio/audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-chunks/chunk1.wav : Hair. \n",
      "Error: \n",
      "Error: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-04 05:44:12,052] ERROR in app: Exception on /api/predict/ [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1359, in _run_fn\n",
      "    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1451, in _call_tf_sessionrun\n",
      "    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\n",
      "  (0) Invalid argument: In[0] ndims must be >= 2: 1\n",
      "\t [[{{node xw_plus_b/MatMul}}]]\n",
      "  (1) Invalid argument: In[0] ndims must be >= 2: 1\n",
      "\t [[{{node xw_plus_b/MatMul}}]]\n",
      "\t [[ArgMax/_77]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/flask_cors/extension.py\", line 165, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/gradio/networking.py\", line 87, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/gradio/networking.py\", line 222, in predict\n",
      "    prediction, durations = app.interface.process(raw_input)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/gradio/interface.py\", line 278, in process\n",
      "    predictions, durations = self.run_prediction(processed_input, return_duration=True)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/gradio/interface.py\", line 252, in run_prediction\n",
      "    prediction = predict_fn(*processed_input)\n",
      "  File \"<ipython-input-11-19937744c486>\", line 85, in audio_prediction\n",
      "    prediction = sess.run(classifier.prediction, feed_dict)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 967, in run\n",
      "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1190, in _run\n",
      "    results = self._do_run(handle, final_targets, final_fetches,\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1368, in _do_run\n",
      "    return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\n",
      "  (0) Invalid argument: In[0] ndims must be >= 2: 1\n",
      "\t [[node xw_plus_b/MatMul (defined at <ipython-input-4-6f7b6961425e>:45) ]]\n",
      "  (1) Invalid argument: In[0] ndims must be >= 2: 1\n",
      "\t [[node xw_plus_b/MatMul (defined at <ipython-input-4-6f7b6961425e>:45) ]]\n",
      "\t [[ArgMax/_77]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "Errors may have originated from an input operation.\n",
      "Input Source operations connected to node xw_plus_b/MatMul:\n",
      " dropout/Mul (defined at <ipython-input-4-6f7b6961425e>:40)\n",
      "\n",
      "Input Source operations connected to node xw_plus_b/MatMul:\n",
      " dropout/Mul (defined at <ipython-input-4-6f7b6961425e>:40)\n",
      "\n",
      "Original stack trace for 'xw_plus_b/MatMul':\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 149, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 787, in inner\n",
      "    self.run()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n",
      "    yield self.process_one()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 225, in wrapper\n",
      "    runner = Runner(result, future, yielded)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 714, in __init__\n",
      "    self.run()\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n",
      "    self.do_execute(\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2877, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2923, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3146, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-a0d7e958c0a1>\", line 2, in <module>\n",
      "    classifier.build_graph()\n",
      "  File \"<ipython-input-4-6f7b6961425e>\", line 45, in build_graph\n",
      "    y_hat = tf.compat.v1.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 4954, in xw_plus_b\n",
      "    mm = math_ops.matmul(x, weights)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\", line 3276, in matmul\n",
      "    return gen_math_ops.batch_mat_mul_v2(\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1534, in batch_mat_mul_v2\n",
      "    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 748, in _apply_op_helper\n",
      "    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3528, in _create_op_internal\n",
      "    ret = Operation(\n",
      "  File \"/home/mridul/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1990, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n"
     ]
    }
   ],
   "source": [
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x57pEkv9UDU8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
